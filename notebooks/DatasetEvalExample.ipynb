{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Evaluation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "import numpy as np\n",
    "import scipy.io as scio\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import quat_math as qm\n",
    "from generic_pose.utils import to_np, to_var\n",
    "\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "pylab.rcParams['figure.figsize'] = 20, 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful display functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshowCV(img, axis = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if(show):\n",
    "        plt.show()\n",
    "    \n",
    "def imshow(img, axis = False, colorbar = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "    if(colorbar):\n",
    "        plt.colorbar()\n",
    "    if(show):\n",
    "        plt.show()\n",
    "    \n",
    "def torch2Img(img, show = True):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    disp_img = to_np(img)\n",
    "    if len(disp_img.shape) == 4:\n",
    "        disp_img = disp_img[0]\n",
    "    disp_img = disp_img.transpose((1,2,0))\n",
    "    disp_img = disp_img * std + mean\n",
    "    return disp_img\n",
    "    \n",
    "#from generic_pose.utils.image_preprocessing import unprocessImages\n",
    "def imshowTorch(img, axis = False, show = True):\n",
    "    if not axis:\n",
    "        plt.axis('off')\n",
    "    disp_img = torch2Img(img)\n",
    "    plt.imshow(disp_img.astype(np.uint8))\n",
    "    #plt.imshow(unprocessImages(img)[0])\n",
    "    if(show):\n",
    "        plt.show()\n",
    "\n",
    "def plotImageScatter(img, choose, show = True):\n",
    "    coords = np.unravel_index(choose, img.shape[:2])    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(img.astype(np.uint8))    \n",
    "    plt.scatter(coords[1], coords[0], 50)\n",
    "    #plt.colorbar()\n",
    "    if(show):\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of YCB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_root = '../datasets/ycb/YCB_Video_Dataset'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location of Dense Fusion Weights File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = '/path/to/model/weights.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inialize Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dense_fusion.evaluate import DenseFusionEstimator, preprocessData\n",
    "from dense_fusion.data_processing import preprocessPoseCNNMetaData\n",
    "\n",
    "num_points = 1000\n",
    "num_obj = 21\n",
    "df_estimator = DenseFusionEstimator(num_points, num_obj, model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from object_pose_utils.datasets.pose_dataset import OutputTypes as otypes\n",
    "\n",
    "output_format = [otypes.OBJECT_LABEL,\n",
    "                 otypes.QUATERNION,\n",
    "                 otypes.IMAGE_CROPPED,\n",
    "                 otypes.DEPTH_POINTS_MASKED_AND_INDEXES]\n",
    "\n",
    "object_list = [14]\n",
    "mode = \"train_syn\"\n",
    "\n",
    "from object_pose_utils.datasets.ycb_dataset import YcbDataset as YCBDataset\n",
    "from object_pose_utils.datasets.ycb_dataset import YcbImagePreprocessor\n",
    "\n",
    "dataset = YCBDataset(dataset_root, mode=mode, \n",
    "                     object_list = object_list, \n",
    "                     output_data = output_format, \n",
    "                     preprocessor = YcbImagePreprocessor,\n",
    "                     image_size = [640, 480], num_points=1000)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14]])\n",
      "tensor([ 0.7652, -0.4055,  0.2423,  0.4375], grad_fn=<IndexBackward>)\n",
      "tensor([[ 0.7850, -0.3913,  0.2590,  0.4044]])\n",
      "tensor([ 0.0571, -0.0190,  0.6155], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "#data = dataset.__getitem__(idx)\n",
    "for data in dataloader:\n",
    "    break\n",
    "obj, q, im, pts, chs = data\n",
    "\n",
    "pred_r, pred_t, pred_c = df_estimator.estimatePose(im, pts, chs, obj-1, return_all=False)\n",
    "pred_r = pred_r[[1,2,3,0]]\n",
    "print(obj)\n",
    "print(pred_r)\n",
    "print(q)\n",
    "print(pred_t)\n",
    "\n",
    "#imshowTorch(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "tensor([ 0.7738, -0.3887,  0.2321,  0.4431], grad_fn=<IndexBackward>)\n",
      "tensor([[ 0.7850, -0.3913,  0.2590,  0.4044]])\n",
      "tensor([ 0.0567, -0.0181,  0.6145], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "from dense_fusion.evaluate_likelihood import getYCBClassData, getYCBGroundtruth\n",
    "\n",
    "test_filenames = '{0}/image_sets/train_split.txt'.format(dataset_root)\n",
    "with open(test_filenames) as f:\n",
    "    file_list = f.read().split()\n",
    "\n",
    "data_prefix = '{0}/data/{1}'.format(dataset_root, file_list[0])\n",
    "\n",
    "img = Image.open('{}-color.png'.format(data_prefix))\n",
    "depth = np.array(Image.open('{}-depth.png'.format(data_prefix)))\n",
    "pose_meta = scio.loadmat('{}-meta.mat'.format(data_prefix))\n",
    "posecnn_meta = scio.loadmat('{}-posecnn.mat'.format(data_prefix))\n",
    "object_classes = set(pose_meta['cls_indexes'].flat) & \\\n",
    "                    set(posecnn_meta['rois'][:,1:2].flatten().astype(int))\n",
    "cls_idx = 14 #list(object_classes)[0]\n",
    "obj_idx = np.nonzero(posecnn_meta['rois'][:,1].astype(int) == cls_idx)[0][0]\n",
    "mask, bbox, object_label = preprocessPoseCNNMetaData(posecnn_meta, obj_idx)\n",
    "\n",
    "q_gt, _ = getYCBGroundtruth(pose_meta, posecnn_meta, obj_idx)\n",
    "q_gt = torch.Tensor(q_gt).unsqueeze(0)\n",
    "if(torch.cuda.is_available()):\n",
    "    q_gt = q_gt.cuda()\n",
    "\n",
    "\n",
    "pred_r, pred_t = df_estimator(img, depth, mask, bbox, object_label)\n",
    "pred_r = pred_r[[1,2,3,0]]\n",
    "print(object_label)\n",
    "print(pred_r)\n",
    "print(q_gt)\n",
    "print(pred_t)\n",
    "#imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bpy_kernel",
   "language": "python",
   "name": "bpy_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
